// Model for an NVIDIA Tesla K40 (Kepler-based GPU)
// This model treats a streaming multiprocessor (SMX) as a core.
// And models the scratchpad (not register file) as the cache.
//
// References:
//   Tesla Kepler GPU accelerators overview
//     http://www.nvidia.com/content/tesla/pdf/Tesla-KSeries-Overview-LR.pdf
//   Inside Kepler
//     http://on-demand.gputechconf.com/gtc/2012/presentations/S0642-GTC2012-Inside-Kepler.pdf
//
//   CUDA Programming Guide 8.0
//
//   Whitepaper Kepler TM GK110
//     http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf
// 
// 192 CUDA core / SMX
// K40 has 15 SMX and 2880 CUDA cores (15 * 192 = 2880)
// 1.43 Double Precision TFLOP/s (peak)
// 4.29 Single Precision TFLOP/s (peak)
// 


// Core Parameters
param kNumCores = 15 // total #SMX's - 15 per
param kCoreClock = 745 * mega in 100 * mega .. 2000 * mega // 745 MHz
param kSIMD = 32
param kNumWarps = 6

// GDDR5 Parameters
param gddr5Clock = 6000 * mega  in 100 * mega .. 6000 * mega // 6000 MHz
param gddr5Width = 48
//param eccPenalty = 0.75 // 133 GB/s vs. 177 GB/s
param eccPenalty = 1 //ECC is off.
param gddr5BW = gddr5Clock * gddr5Width * eccPenalty
param gddr5Lat = 50 * nano / kSIMD / kNumWarps / 64 ///\todo; DIVING BY FSIMD IS A HACK!!
param gddr5Cap = 12 * giga

// Power
param fIdle = 30 //need to update
param fTDP = 235

socket nvidia_k40 {
   core [kNumCores] keplerCore 
   memory gddr5
   cache fermiSP
   link pcie

   static power [fIdle]
}

core keplerCore {
   resource flops(number) [number / kCoreClock / kNumWarps / 8]
     with dp   [base * 3], 
          simd [base / kSIMD], 
          fmad [base / 2],
          sin  [base * 18]
     dynamic power [(fTDP - fIdle) / kNumCores]
}

memory gddr5 {
   property capacity  [gddr5Cap] 
   
   ///\todo: stride used to be ddr3BW/2.  I can only assume
   /// it should have been "base*2" (which I'm now using), or else
   /// the full expression ddr3Lat + numBytes / (ddr2BW/2).
   ///\todo: random used to be 3*giga.  I'm not sure the 
   /// intent, so I'm using base*8 instead.
   resource loads(numBytes)  [gddr5Lat + (numBytes / gddr5BW)]
     with stride [base*1.2], random[base*1.5]
     dynamic power [fTDP - fIdle]

   resource stores(numBytes) [(gddr5Lat + (numBytes / gddr5BW))*1.1]
     with stride [base*1.3], random[base*2]
     dynamic power [fTDP - fIdle]

   conflict loads, stores
   ///\todo: having loads/stores conflict with flops drastically
   /// improved the accuracy of the predictions for a matmul example:
   //conflict loads, flops
}

// Fermi Scratchpad cache
cache fermiSP {
  // bw = 1030 GB/s aggregate / 15 SM's
  // Shared memory bandwidth:
  // 
  property capacity [32 * kilo]
  property latency [0]
  property bandwidth [1177 * giga / 15]
}

link pcie {
  property latency [5 * micro]
  property bandwidth [6 * giga]
   
  ///\todo: We should be able to use the above properties here, as follows:
  //resource intracomm(bytes) [latency + (bytes/bandwidth)]
  //resource intracomm(bytes) [5*micro + (bytes/(6*giga))]

  // Jeremy, 2013-09-09: measured latency is nothing like 5 us.
  // Using SHOC on my workstation: PCIe v3 to GTX680 is 15 us (pinned or not)
  //                               PCIe v2 to C2050  is 30 us (pinned or not)
  //                               and, oddly, the 9500GT is only about 15 us as well....
  resource intracomm(bytes) [15*micro + (bytes/(6*giga))]
}
